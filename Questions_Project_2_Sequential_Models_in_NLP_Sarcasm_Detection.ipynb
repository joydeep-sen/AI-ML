{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Questions - Project 2 - Sequential Models in NLP - Sarcasm Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7SIeJ_oZMU"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1tpOCamr9aWz817atPnyXus8w5gJ3mIts\" width=500px>\n",
        "\n",
        "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eukag7wEoPZu"
      },
      "source": [
        "### Package Version:\n",
        "- tensorflow==2.2.0\n",
        "- pandas==1.0.5\n",
        "- numpy==1.18.5\n",
        "- google==2.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp68FAQf9aMN"
      },
      "source": [
        "# Sarcasm Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEahVPtWX5ve"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "#### Acknowledgement\n",
        "Misra, Rishabh, and Prahal Arora. \"Sarcasm Detection using Hybrid Neural Network.\" arXiv preprint arXiv:1908.07414 (2019).\n",
        "\n",
        "**Required Files given in below link.**\n",
        "\n",
        "https://drive.google.com/drive/folders/1xUnF35naPGU63xwRDVGc-DkZ3M8V5mMk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAk6BRUh8CqL"
      },
      "source": [
        "### Load Data (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8-PQsV0DrAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327c8173-11a7-4322-bb5e-cee5970d38ae"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNDniZfJBW7M"
      },
      "source": [
        "import json \r\n",
        "\r\n",
        "data = []\r\n",
        "for line in open('/content/drive/My Drive/AIML/Labs/Datasets/Sarcasm/Sarcasm_Headlines_Dataset.json','r'):\r\n",
        "  data.append(json.loads(line))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiM1Ul_XNR1I"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.DataFrame(data, columns = ['article_link','headline','is_sarcastic'])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6pXf7A78E2H"
      },
      "source": [
        "### Drop `article_link` from dataset (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUNHq5zEV0n"
      },
      "source": [
        "df = df.drop(['article_link'], axis = 1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0h6IOxU8OdH"
      },
      "source": [
        "### Get length of each headline and add a column for that (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLpiBRDmEV2l"
      },
      "source": [
        "length = [len(i) for i in df['headline']]\r\n",
        "df['length'] = length"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMF-wjJ2aMwm"
      },
      "source": [
        "### Initialize parameter values\n",
        "- Set values for max_features, maxlen, & embedding_size\n",
        "- max_features: Number of words to take from tokenizer(most frequent words)\n",
        "- maxlen: Maximum length of each sentence to be limited to 25\n",
        "- embedding_size: size of embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPw9gAN_EV6m"
      },
      "source": [
        "max_features = 10000\n",
        "maxlen = 25\n",
        "embedding_size = 200"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abSe-bM8fn9"
      },
      "source": [
        "### Apply `tensorflow.keras` Tokenizer and get indices for words (5 Marks)\n",
        "- Initialize Tokenizer object with number of words as 10000\n",
        "- Fit the tokenizer object on headline column\n",
        "- Convert the text to sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAyoBhmi40xb"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "df['headline'] = df['headline'].apply(lambda x: x.lower())\r\n",
        "df['headline'] = df['headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKJaYI5s58xL"
      },
      "source": [
        "for idx, row in df.iterrows():\r\n",
        "    row[0] = row[0].replace('rt',' ')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8g4l0KfF3eh"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words = max_features, split= ' ')\r\n",
        "tokenizer.fit_on_texts(df['headline'].values)\r\n",
        "\r\n",
        "X = tokenizer.texts_to_sequences(df['headline'].values)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeZpwPO4bOkZ"
      },
      "source": [
        "### Pad sequences (5 Marks)\n",
        "- Pad each example with a maximum length\n",
        "- Convert target column into numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV0K70E5c9Xl"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "X = pad_sequences(X, maxlen = maxlen, padding = 'post', value = 0)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFgLKTjewQxM"
      },
      "source": [
        "y = df['is_sarcastic'].values"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svUzXzxg_4AQ",
        "outputId": "0ad29423-22c8-484b-ff0a-aa0551a2207c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 42)\r\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21367, 25), (5342, 25), (21367,), (5342,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJLyKg-98rH_"
      },
      "source": [
        "### Vocab mapping\n",
        "- There is no word for 0th index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45g923squZbY"
      },
      "source": [
        "# tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRiNX58Rb3oJ"
      },
      "source": [
        "### Set number of words\n",
        "- Since the above 0th index doesn't have a word, add 1 to the length of the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfwq6ou8ck2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b087fe85-2b29-4784-b789-6a0c52bb4d73"
      },
      "source": [
        "num_words = len(tokenizer.word_index) + 1\n",
        "print(num_words)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUF1TuQa8ux0"
      },
      "source": [
        "### Load Glove Word Embeddings (5 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prHSzdQUcZhm"
      },
      "source": [
        "### Create embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elZ-T5aFGZmZ"
      },
      "source": [
        "EMBEDDING_FILE = '/content/drive/My Drive/AIML/Labs/Datasets/Sarcasm/glove.6B.200d.txt'\n",
        "\n",
        "embeddings = {}\n",
        "for o in open(EMBEDDING_FILE):\n",
        "    word = o.split(\" \")[0]\n",
        "    # print(word)\n",
        "    embd = o.split(\" \")[1:]\n",
        "    embd = np.asarray(embd, dtype='float32')\n",
        "    # print(embd)\n",
        "    embeddings[word] = embd\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "\tembedding_vector = embeddings.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7IbWuEX82Ra"
      },
      "source": [
        "### Define model (10 Marks)\n",
        "- Hint: Use Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, flatten it, then dense and dropout layers as required. \n",
        "In the end add a final dense layer with sigmoid activation for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tv168Gmc3PY"
      },
      "source": [
        "import keras as k\r\n",
        "\r\n",
        "model = k.models.Sequential()\r\n",
        "model.add(k.layers.Embedding(num_words, 200, embeddings_initializer=k.initializers.Constant(embedding_matrix), weights = [embedding_matrix], trainable = False))\r\n",
        "model.add(k.layers.Bidirectional(k.layers.LSTM(units = 64, return_sequences = True)))\r\n",
        "model.add(k.layers.Dense(64))\r\n",
        "model.add(k.layers.Dropout(0.25))\r\n",
        "model.add(k.layers.GlobalMaxPool1D())\r\n",
        "model.add(k.layers.Flatten())\r\n",
        "model.add(k.layers.Dense(units = 1, activation='sigmoid'))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoI7_8Y1cqTj"
      },
      "source": [
        "### Compile the model (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jJiPHeNoJ3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b45924-746f-4c63-9e64-66c7f5bf29ba"
      },
      "source": [
        "model.compile(optimizer=k.optimizers.Adam(), loss = 'binary_crossentropy', metrics=['acc'])\r\n",
        "print(model.summary())"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 200)         5679800   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 128)         135680    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 64)          8256      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,823,801\n",
            "Trainable params: 144,001\n",
            "Non-trainable params: 5,679,800\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s4nmqcecw3a"
      },
      "source": [
        "### Fit the model (5 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN789zNnJ5PL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9754c71d-0702-4b20-e929-828d20260af4"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)\r\n",
        "model.fit(X_train, y_train, epochs = 3, batch_size = 32)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "  5/668 [..............................] - ETA: 17s - loss: 0.1999 - acc: 0.9125"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
            "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "668/668 [==============================] - 18s 28ms/step - loss: 0.2078 - acc: 0.9151\n",
            "Epoch 2/3\n",
            "668/668 [==============================] - 19s 28ms/step - loss: 0.1617 - acc: 0.9366\n",
            "Epoch 3/3\n",
            "668/668 [==============================] - 19s 28ms/step - loss: 0.1191 - acc: 0.9561\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff9ecd9ac88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch94nztGAqRG",
        "outputId": "e878d1e5-dff4-4303-9d82-aa0d8c0be586"
      },
      "source": [
        "acc = model.evaluate(X_test, y_test, verbose = 2, batch_size = 32)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
            "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "167/167 - 2s - loss: 0.2127 - acc: 0.9096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRQcq3v7AC4J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}