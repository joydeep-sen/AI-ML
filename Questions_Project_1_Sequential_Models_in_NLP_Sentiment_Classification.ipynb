{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Questions - Project 1 - Sequential Models in NLP - Sentiment Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXaFSkUu0fzm"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1UXScsVx_Wni_JuDdB8LeTnM6jsPfIwkW)\n",
        "\n",
        "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OudB5by50jlI"
      },
      "source": [
        "# Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "### Dataset\n",
        "- Dataset of 50,000 movie reviews from IMDB, labeled by sentiment positive (1) or negative (0)\n",
        "- Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers).\n",
        "- For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
        "- As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
        "\n",
        "Command to import data\n",
        "- `from tensorflow.keras.datasets import imdb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q34-Y3nRKXdO"
      },
      "source": [
        "### Import the data (4 Marks)\n",
        "- Use `imdb.load_data()` method\n",
        "- Get train and test set\n",
        "- Take 10000 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxfwbrbuKbk2",
        "outputId": "5a128ead-409d-4140-990b-0ae798515936"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = 10000)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DldivBO4LTbP"
      },
      "source": [
        "### Pad each sentence to be of same length (4 Marks)\n",
        "- Take maximum sequence length as 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E808XB4tLtic"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "X_train_p = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen = 300, dtype = 'int32', truncating = 'pre', padding = 'pre', value = 0.0)\r\n",
        "X_test_p = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen = 300, dtype = 'int32', truncating = 'pre', padding = 'pre', value = 0.0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBFFCrybMSXz"
      },
      "source": [
        "### Print shape of features & labels (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOcyRtZfMYZd"
      },
      "source": [
        "Number of review, number of words in each review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdMCUPr7RaCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df8e961-01bf-4dec-b846-c237f31a0cc9"
      },
      "source": [
        "print('Train reviews shape', X_train_p.shape)\r\n",
        "print('Unique Train words', len(np.unique(np.hstack(X_train_p))))\r\n",
        "length = [len(i) for i in X_train]\r\n",
        "print('Average length of original reviews', np.mean(length))\r\n",
        "length = [len(i) for i in X_train_p]\r\n",
        "print('Average length of padded reviews', np.mean(length))\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train reviews shape (25000, 300)\n",
            "Unique Train words 9999\n",
            "Average length of original reviews 238.71364\n",
            "Average length of padded reviews 300.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGVHeKOWyJiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfcb3660-5680-417f-ea99-56fa045b1c75"
      },
      "source": [
        "print('Test reviews shape', X_test_p.shape)\r\n",
        "print('Unique Test words', len(np.unique(np.hstack(X_test_p))))\r\n",
        "length = [len(i) for i in X_test]\r\n",
        "print('Average length of original reviews', np.mean(length))\r\n",
        "length = [len(i) for i in X_test_p]\r\n",
        "print('Average length of padded reviews', np.mean(length))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test reviews shape (25000, 300)\n",
            "Unique Test words 9943\n",
            "Average length of original reviews 230.8042\n",
            "Average length of padded reviews 300.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cNk5sDvMr3j"
      },
      "source": [
        "Number of labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z00-mYgMoKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da50647e-ea11-45c8-d453-2892eee5e6b5"
      },
      "source": [
        "print('Train label counts', y_train.shape)\r\n",
        "print('Unique Train Sentiments', np.unique(y_train))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train label counts (25000,)\n",
            "Unique Train Sentiments [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7f5tPeaMxti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde78c8f-26a5-4000-86df-1646205c156c"
      },
      "source": [
        "print('Test label counts', y_test.shape)\r\n",
        "print('Unique Test Sentiments', np.unique(y_test))\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test label counts (25000,)\n",
            "Unique Test Sentiments [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdXPWuOmNEbh"
      },
      "source": [
        "### Print value of any one feature and it's label (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGLEdeFmNZfR"
      },
      "source": [
        "Feature value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKFyMa28zztL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854e2f24-f58c-4b8e-cec9-4f35b8cc94a5"
      },
      "source": [
        "print(X_train[10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 785, 189, 438, 47, 110, 142, 7, 6, 7475, 120, 4, 236, 378, 7, 153, 19, 87, 108, 141, 17, 1004, 5, 2, 883, 2, 23, 8, 4, 136, 2, 2, 4, 7475, 43, 1076, 21, 1407, 419, 5, 5202, 120, 91, 682, 189, 2818, 5, 9, 1348, 31, 7, 4, 118, 785, 189, 108, 126, 93, 2, 16, 540, 324, 23, 6, 364, 352, 21, 14, 9, 93, 56, 18, 11, 230, 53, 771, 74, 31, 34, 4, 2834, 7, 4, 22, 5, 14, 11, 471, 9, 2, 34, 4, 321, 487, 5, 116, 15, 6584, 4, 22, 9, 6, 2286, 4, 114, 2679, 23, 107, 293, 1008, 1172, 5, 328, 1236, 4, 1375, 109, 9, 6, 132, 773, 2, 1412, 8, 1172, 18, 7865, 29, 9, 276, 11, 6, 2768, 19, 289, 409, 4, 5341, 2140, 2, 648, 1430, 2, 8914, 5, 27, 3000, 1432, 7130, 103, 6, 346, 137, 11, 4, 2768, 295, 36, 7740, 725, 6, 3208, 273, 11, 4, 1513, 15, 1367, 35, 154, 2, 103, 2, 173, 7, 12, 36, 515, 3547, 94, 2547, 1722, 5, 3547, 36, 203, 30, 502, 8, 361, 12, 8, 989, 143, 4, 1172, 3404, 10, 10, 328, 1236, 9, 6, 55, 221, 2989, 5, 146, 165, 179, 770, 15, 50, 713, 53, 108, 448, 23, 12, 17, 225, 38, 76, 4397, 18, 183, 8, 81, 19, 12, 45, 1257, 8, 135, 15, 2, 166, 4, 118, 7, 45, 2, 17, 466, 45, 2, 4, 22, 115, 165, 764, 6075, 5, 1030, 8, 2973, 73, 469, 167, 2127, 2, 1568, 6, 87, 841, 18, 4, 22, 4, 192, 15, 91, 7, 12, 304, 273, 1004, 4, 1375, 1172, 2768, 2, 15, 4, 22, 764, 55, 5773, 5, 14, 4233, 7444, 4, 1375, 326, 7, 4, 4760, 1786, 8, 361, 1236, 8, 989, 46, 7, 4, 2768, 45, 55, 776, 8, 79, 496, 98, 45, 400, 301, 15, 4, 1859, 9, 4, 155, 15, 66, 2, 84, 5, 14, 22, 1534, 15, 17, 4, 167, 2, 15, 75, 70, 115, 66, 30, 252, 7, 618, 51, 9, 2161, 4, 3130, 5, 14, 1525, 8, 6584, 15, 2, 165, 127, 1921, 8, 30, 179, 2532, 4, 22, 9, 906, 18, 6, 176, 7, 1007, 1005, 4, 1375, 114, 4, 105, 26, 32, 55, 221, 11, 68, 205, 96, 5, 4, 192, 15, 4, 274, 410, 220, 304, 23, 94, 205, 109, 9, 55, 73, 224, 259, 3786, 15, 4, 22, 528, 1645, 34, 4, 130, 528, 30, 685, 345, 17, 4, 277, 199, 166, 281, 5, 1030, 8, 30, 179, 4442, 444, 2, 9, 6, 371, 87, 189, 22, 5, 31, 7, 4, 118, 7, 4, 2068, 545, 1178, 829]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_85Hqm0Nb1I"
      },
      "source": [
        "Label value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FoehB5jNd1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968cb6ef-bbde-46c0-a0ea-5ba005e694d3"
      },
      "source": [
        "print(y_train[10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cof4LSxNxuv"
      },
      "source": [
        "### Decode the feature value to get original sentence (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_oiAyPZOkJD"
      },
      "source": [
        "First, retrieve a dictionary that contains mapping of words to their index in the IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clsk-yK8OtzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3361092-2c9c-4e23-9554-0623b380315f"
      },
      "source": [
        "index = imdb.get_word_index()\r\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRgOD5S2Uuvd"
      },
      "source": [
        "Now use the dictionary to get the original words from the encodings, for a particular sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ504QDORwxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9d8c20-aa6c-4ed8-c82a-0b580d9dac9c"
      },
      "source": [
        "decoded = ' '.join([reverse_index.get(i - 3, '#') for i in X_train[10]])\r\n",
        "print(decoded)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# french horror cinema has seen something of a revival over the last couple of years with great films such as inside and # romance # on to the scene # # the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made # was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is # by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named # sent to prison for fraud he is put in a cell with three others the quietly insane # body building # marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old # after # part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that # makes the best of it's # as despite it's # the film never actually feels restrained and manages to flow well throughout director eric # provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell # that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really # people and this film proves that as the director # that we can never really be sure of exactly what is round the corner and this helps to ensure that # actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall # is a truly great horror film and one of the best of the decade highly recommended viewing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLGABrJoVZe6"
      },
      "source": [
        "Get the sentiment for the above sentence\n",
        "- positive (1)\n",
        "- negative (0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDyQGJT0Ve-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d744965c-1965-43dc-d5be-d7abebb0325b"
      },
      "source": [
        "print('Positive' if y_train[10] == 1 else 'Negative')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmCjr8miXIWB"
      },
      "source": [
        "### Define model (10 Marks)\n",
        "- Define a Sequential Model\n",
        "- Add Embedding layer\n",
        "  - Embedding layer turns positive integers into dense vectors of fixed size\n",
        "  - `tensorflow.keras` embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unique integer number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn LabelEncoder.\n",
        "  - Size of the vocabulary will be 10000\n",
        "  - Give dimension of the dense embedding as 100\n",
        "  - Length of input sequences should be 300\n",
        "- Add LSTM layer\n",
        "  - Pass value in `return_sequences` as True\n",
        "- Add a `TimeDistributed` layer with 100 Dense neurons\n",
        "- Add Flatten layer\n",
        "- Add Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np5GxT1caFEq"
      },
      "source": [
        "import keras as k"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0qiwxVRJe1_"
      },
      "source": [
        "e_init = k.initializers.RandomUniform(-0.01, 0.01, seed = 1)\r\n",
        "init = k.initializers.glorot_uniform(seed = 1)\r\n",
        "adam = k.optimizers.Adam()\r\n",
        "embed_len = 100\r\n",
        "\r\n",
        "model = k.models.Sequential()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f5uxlyKKPS4",
        "outputId": "3855a13c-6528-4327-b6fd-f2d4b53504bf"
      },
      "source": [
        "model.add(k.layers.embeddings.Embedding(input_dim = 10000, input_length=300, output_dim=100, embeddings_initializer=e_init))\r\n",
        "model.add(k.layers.LSTM(units = 100, kernel_initializer=init, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\r\n",
        "model.add(k.layers.TimeDistributed(k.layers.Dense(100)))\r\n",
        "model.add(k.layers.Flatten())\r\n",
        "model.add(k.layers.Dense(units=1, kernel_initializer=init, activation='sigmoid'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc4bknOobDby"
      },
      "source": [
        "### Compile the model (4 Marks)\n",
        "- Use Optimizer as Adam\n",
        "- Use Binary Crossentropy as loss\n",
        "- Use Accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw4RJ0CQbwFY"
      },
      "source": [
        "model.compile(optimizer=adam, loss = 'binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sEzwazqbz3T"
      },
      "source": [
        "### Print model summary (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx1yxwlb2Ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9768bf8a-e264-48c0-86f2-eca6511f6e17"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 300, 100)          80400     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 300, 100)          10100     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 30000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 30001     \n",
            "=================================================================\n",
            "Total params: 1,120,501\n",
            "Trainable params: 1,120,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmkolKP4b-U6"
      },
      "source": [
        "### Fit the model (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRg3KFXLcAkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4296771-e1f5-4369-e518-b51e3c794558"
      },
      "source": [
        "model.fit(X_train_p, y_train, epochs = 5, batch_size = 128)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 243s 1s/step - loss: 0.0991 - acc: 0.9638\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 237s 1s/step - loss: 0.0499 - acc: 0.9819\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 239s 1s/step - loss: 0.0283 - acc: 0.9896\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 237s 1s/step - loss: 0.0185 - acc: 0.9935\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 237s 1s/step - loss: 0.0185 - acc: 0.9939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f97153e2400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwLl54MXnkEA"
      },
      "source": [
        "### Evaluate model (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUqY-bD8RaDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2658ec-a2ea-4ff9-8a0b-1a3b67ecaafb"
      },
      "source": [
        "model_acc = model.evaluate(X_test_p, y_test, verbose = 1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 74s 94ms/step - loss: 0.9092 - acc: 0.8558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2amr1tJn9Jz"
      },
      "source": [
        "### Predict on one sample (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl4idfWR_A8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f22046-abbe-4339-8263-0cea39c9db25"
      },
      "source": [
        "decoded = ' '.join([reverse_index.get(i - 3, '#') for i in X_test[1786]])\r\n",
        "print(decoded)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# hi i'm # an african yet white jungle # princess who possesses the incredible ability to # into the # monster in the world think 60s star trek aliens by rolling # in mud when i first found myself in this horrible position i took the only logical action i made myself a torn apart jungle bikini in which to perform my badly acted antics i enjoy romance novels and # apart the occasional unimpressive african # and i would be # if i did not mention my white of course sidekick mr cutter an american ex military man who seems to have # the u s after his divorce can you say # # anyway he provides the occasional distraction from my difficult life i mean how many idiot # do you know who are also an # species of flesh # monster despite my many # acting is so hard # i haven't given up and after much soul searching i have finally discovered my role in life to # late night television viewers who are so unfortunate as to not have cable or satellite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdbXlqq17W6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b3c825-5b29-4308-cf9f-f4edc7e06fca"
      },
      "source": [
        "print('Predicted Positive' if model.predict(X_test_p[1786].reshape(1,300)) == 1 else 'Predicted Negative')\r\n",
        "print('Actual Positive' if y_test[1786] == 1 else 'Actual Negative')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Negative\n",
            "Actual Negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}